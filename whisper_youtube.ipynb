{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Youtube Videos Transcription with OpenAI's Whisper**\n",
        "\n",
        "[![blog post shield](https://img.shields.io/static/v1?label=&message=Blog%20post&color=blue&style=for-the-badge&logo=openai&link=https://openai.com/blog/whisper)](https://openai.com/blog/whisper)\n",
        "[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)](https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)\n",
        "[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/openai/whisper)](https://github.com/openai/whisper)\n",
        "[![paper shield](https://img.shields.io/static/v1?label=&message=Paper&color=blue&style=for-the-badge&link=https://cdn.openai.com/papers/whisper.pdf)](https://cdn.openai.com/papers/whisper.pdf)\n",
        "[![model card shield](https://img.shields.io/static/v1?label=&message=Model%20card&color=blue&style=for-the-badge&link=https://github.com/openai/whisper/blob/main/model-card.md)](https://github.com/openai/whisper/blob/main/model-card.md)\n",
        "\n",
        "Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\n",
        "\n",
        "This Notebook will guide you through the transcription of a Youtube video using Whisper. You'll be able to explore most inference parameters or use the Notebook as-is to store the transcript and video audio in your Google Drive."
      ],
      "metadata": {
        "id": "96kvih9mXkNN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Check GPU type** üïµÔ∏è\n",
        "\n",
        "#@markdown The type of GPU you get assigned in your Colab session defined the speed at which the video will be transcribed.\n",
        "#@markdown The higher the number of floating point operations per second (FLOPS), the faster the transcription.\n",
        "#@markdown But even the least powerful GPU available in Colab is able to run any Whisper model.\n",
        "#@markdown Make sure you've selected `GPU` as hardware accelerator for the Notebook (Runtime &rarr; Change runtime type &rarr; Hardware accelerator).\n",
        "\n",
        "#@markdown |  GPU   |  GPU RAM   | FP32 teraFLOPS |     Availability   |\n",
        "#@markdown |:------:|:----------:|:--------------:|:------------------:|\n",
        "#@markdown |  T4    |    16 GB   |       8.1      |         Free       |\n",
        "#@markdown | P100   |    16 GB   |      10.6      |      Colab Pro     |\n",
        "#@markdown | V100   |    16 GB   |      15.7      |  Colab Pro (Rare)  |\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown **Factory reset your Notebook's runtime if you want to get assigned a new GPU.**\n",
        "\n",
        "!nvidia-smi -L\n",
        "\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QshUbLqpX7L4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IfG0E_WbRFI0",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Install libraries** üèóÔ∏è\n",
        "#@markdown This cell will take a little while to download several libraries, including Whisper.\n",
        "\n",
        "#@markdown ---\n",
        "\n",
        "! pip install git+https://github.com/openai/whisper.git\n",
        "! pip install yt-dlp\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "import whisper\n",
        "from pathlib import Path\n",
        "import yt_dlp\n",
        "import subprocess\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown, YouTubeVideo\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zwGAsr4sIgd",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown # **Optional:** Save data in Google Drive üíæ\n",
        "#@markdown Enter a Google Drive path and run this cell if you want to store the results inside Google Drive.\n",
        "\n",
        "# Uncomment to copy generated images to drive, faster than downloading directly from colab in my experience.\n",
        "from google.colab import drive\n",
        "drive_mount_path = Path(\"/\") / \"content\" / \"drive\"\n",
        "drive.mount(str(drive_mount_path))\n",
        "drive_mount_path /= \"My Drive\"\n",
        "#@markdown ---\n",
        "drive_path = \"Colab Notebooks/Whisper Youtube\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change your Google Drive path.**\n",
        "\n",
        "drive_whisper_path = drive_mount_path / Path(drive_path.lstrip(\"/\"))\n",
        "drive_whisper_path.mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Model selection** üß†\n",
        "\n",
        "#@markdown As of the first public release, there are 4 pre-trained options to play with:\n",
        "\n",
        "#@markdown |  Size  | Parameters | English-only model | Multilingual model | Required VRAM | Relative speed |\n",
        "#@markdown |:------:|:----------:|:------------------:|:------------------:|:-------------:|:--------------:|\n",
        "#@markdown |  tiny  |    39 M    |     `tiny.en`      |       `tiny`       |     ~1 GB     |      ~32x      |\n",
        "#@markdown |  base  |    74 M    |     `base.en`      |       `base`       |     ~1 GB     |      ~16x      |\n",
        "#@markdown | small  |   244 M    |     `small.en`     |      `small`       |     ~2 GB     |      ~6x       |\n",
        "#@markdown | medium |   769 M    |    `medium.en`     |      `medium`      |     ~5 GB     |      ~2x       |\n",
        "#@markdown | large  |   1550 M   |        N/A         |      `large`       |    ~10 GB     |       1x       |\n",
        "\n",
        "#@markdown ---\n",
        "Model = 'medium' #@param ['tiny.en', 'tiny', 'base.en', 'base', 'small.en', 'small', 'medium.en', 'medium', 'large']\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the model.**\n",
        "\n",
        "whisper_model = whisper.load_model(Model)\n",
        "\n",
        "if Model in whisper.available_models():\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is selected.**\"\n",
        "    ))\n",
        "else:\n",
        "    display(Markdown(\n",
        "        f\"**{Model} model is no longer available.**<br /> Please select one of the following:<br /> - {'<br /> - '.join(whisper.available_models())}\"\n",
        "    ))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "TMhrSq_GZ6kA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # **Video selection** üì∫\n",
        "\n",
        "#@markdown Enter the URL of the Youtube video you want to transcribe, wether you want to save the audio file in your Google Drive, and run the cell.\n",
        "\n",
        "Type = \"Youtube video or playlist\" #@param ['Youtube video or playlist', 'Google Drive']\n",
        "#@markdown ---\n",
        "#@markdown #### **Youtube video or playlist**\n",
        "URL = \"https://youtu.be/L_Guz73e6fw\" #@param {type:\"string\"}\n",
        "# store_audio = True #@param {type:\"boolean\"}\n",
        "#@markdown ---\n",
        "#@markdown #### **Google Drive video, audio (mp4, wav), or folder containing video and/or audio files**\n",
        "video_path = \"Colab Notebooks/transcription/my_video.mp4\" #@param {type:\"string\"}\n",
        "#@markdown ---\n",
        "#@markdown **Run this cell again if you change the video.**\n",
        "\n",
        "video_path_local_list = []\n",
        "\n",
        "if Type == \"Youtube video or playlist\":\n",
        "\n",
        "    ydl_opts = {\n",
        "        'format': 'm4a/bestaudio/best',\n",
        "        'outtmpl': '%(id)s.%(ext)s',\n",
        "        # ‚ÑπÔ∏è See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "        'postprocessors': [{  # Extract audio using ffmpeg\n",
        "            'key': 'FFmpegExtractAudio',\n",
        "            'preferredcodec': 'wav',\n",
        "        }]\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        error_code = ydl.download([URL])\n",
        "        list_video_info = [ydl.extract_info(URL, download=False)]\n",
        "\n",
        "    for video_info in list_video_info:\n",
        "        video_path_local_list.append(Path(f\"{video_info['id']}.wav\"))\n",
        "\n",
        "elif Type == \"Google Drive\":\n",
        "    # video_path_drive = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "    video_path = drive_mount_path / Path(video_path.lstrip(\"/\"))\n",
        "    if video_path.is_dir():\n",
        "        for video_path_drive in video_path.glob(\"**/*\"):\n",
        "            if video_path_drive.is_file():\n",
        "                display(Markdown(f\"**{str(video_path_drive)} selected for transcription.**\"))\n",
        "            elif video_path_drive.is_dir():\n",
        "                display(Markdown(f\"**Subfolders not supported.**\"))\n",
        "            else:\n",
        "                display(Markdown(f\"**{str(video_path_drive)} does not exist, skipping.**\"))\n",
        "            video_path_local = Path(\".\").resolve() / (video_path_drive.name)\n",
        "            shutil.copy(video_path_drive, video_path_local)\n",
        "            video_path_local_list.append(video_path_local)\n",
        "    elif video_path.is_file():\n",
        "        video_path_local = Path(\".\").resolve() / (video_path.name)\n",
        "        shutil.copy(video_path, video_path_local)\n",
        "        video_path_local_list.append(video_path_local)\n",
        "        display(Markdown(f\"**{str(video_path)} selected for transcription.**\"))\n",
        "    else:\n",
        "        display(Markdown(f\"**{str(video_path)} does not exist.**\"))\n",
        "\n",
        "else:\n",
        "    raise(TypeError(\"Please select supported input type.\"))\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    if video_path_local.suffix == \".mp4\":\n",
        "        video_path_local = video_path_local.with_suffix(\".wav\")\n",
        "        result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n"
      ],
      "metadata": {
        "id": "xYLPZQX9S7tU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-X0qB9JAzMLY",
        "cellView": "form",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "362b9108-496c-4edc-b05f-ea13db63da03"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": "### Mde2q7GFCrw.wav",
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[00:00.000 --> 00:07.200]  If we now find ourselves inside this kind of world of illusions, created by an alien intelligence\n",
            "[00:07.200 --> 00:16.240]  that we don't understand, but it understands us, this is a kind of spiritual enslavement\n",
            "[00:16.240 --> 00:23.840]  that we won't be able to break out of because it understands us, it understands how to manipulate\n",
            "[00:23.840 --> 00:33.520]  us, but we don't understand what is behind this screen of stories and images and songs.\n",
            "[00:36.800 --> 00:42.960]  The following is a conversation with Yuval Noah Harari, a historian, philosopher, and author of\n",
            "[00:42.960 --> 00:49.440]  several highly acclaimed, highly influential books, including Sapiens, Homo Deus, and 21 Lessons for\n",
            "[00:49.440 --> 00:57.280]  the 21st Century. He is also an outspoken critic of Benjamin Netanyahu and the current right-wing\n",
            "[00:57.280 --> 01:02.320]  government in Israel. So while much of this conversation is about the history and future\n",
            "[01:02.320 --> 01:07.680]  of human civilization, we also discuss the political turmoil of present-day Israel,\n",
            "[01:07.680 --> 01:13.840]  providing a different perspective from that of my recent conversation with Benjamin Netanyahu.\n",
            "[01:14.800 --> 01:19.520]  This is the Lex Friedman Podcast. To support it, please check out our sponsors in the description.\n",
            "[01:19.520 --> 01:26.960]  And now, dear friends, here's Yuval Noah Harari. 13.8 billion years ago is the origin of our\n",
            "[01:26.960 --> 01:33.440]  universe. 3.8 billion years ago is the origin of life here on our little planet, the one we call\n",
            "[01:33.440 --> 01:41.200]  Earth. Let's say 200,000 years ago is the appearance of early Homo sapiens. So let me ask you this\n",
            "[01:41.200 --> 01:46.960]  question. How rare are these events in the vastness of space and time? Or put it in a more\n",
            "[01:46.960 --> 01:50.960]  fun way, how many intelligent alien civilizations do you think are out there in this universe,\n",
            "[01:52.000 --> 01:57.040]  us being one of them? I suppose there should be some, statistically, but we don't have any\n",
            "[01:57.040 --> 02:02.240]  evidence. But I do think that, you know, intelligence in any way, it's a bit overvalued.\n",
            "[02:03.120 --> 02:09.760]  We are the most intelligent entities on this planet, and look what we're doing.\n",
            "[02:10.800 --> 02:18.160]  So intelligence also tends to be self-destructive, which implies that if there are or were\n",
            "[02:18.160 --> 02:22.320]  intelligent life forms elsewhere, maybe they don't survive for long.\n",
            "[02:22.320 --> 02:26.080]  LBW So you think there's a tension between happiness and intelligence?\n",
            "[02:26.800 --> 02:34.080]  Absolutely. Intelligence is definitely not something that is directed towards\n",
            "[02:34.080 --> 02:40.000]  amplifying happiness. I would also emphasize the huge, huge difference between intelligence and\n",
            "[02:40.000 --> 02:46.640]  consciousness, which many people certainly in the tech industry and in the AI industry tend to miss.\n",
            "[02:47.520 --> 02:55.760]  Intelligence is simply the ability to solve problems, to attain goals, and, you know, to win\n",
            "[02:55.760 --> 03:04.400]  a chess, to win a struggle for survival, to win a war, to drive a car, to diagnose a disease. This\n",
            "[03:04.400 --> 03:11.760]  is intelligence. Consciousness is the ability to feel things like pain and pleasure and love and\n",
            "[03:11.760 --> 03:18.560]  hate. In humans and other animals, intelligence and consciousness go together. They go hand in\n",
            "[03:18.560 --> 03:25.680]  hand, which is why we confuse them. We solve problems. We attain goals by having feelings.\n",
            "[03:27.040 --> 03:32.400]  But other types of intelligence, certainly in computers, computers are already highly\n",
            "[03:32.400 --> 03:38.560]  intelligent, and as far as we know, they have zero consciousness. When a computer beats you a\n",
            "[03:38.560 --> 03:44.000]  chess or a goal or whatever, it doesn't feel happy. If it loses, it doesn't feel sad.\n",
            "[03:44.800 --> 03:52.880]  And there could be also other highly intelligent entities out there in the universe that have\n",
            "[03:52.880 --> 03:58.560]  zero consciousness. And I think that consciousness is far more important and valuable than\n",
            "[03:58.560 --> 04:05.040]  intelligence. LBW Can you still make the case that consciousness and intelligence are intricately\n",
            "[04:05.040 --> 04:10.480]  connected? So not just in humans, but anywhere else. They have to go hand in hand. Is it possible\n",
            "[04:10.480 --> 04:16.000]  for you to imagine such a universe? YG It could be, but we don't know yet. Again,\n",
            "[04:16.000 --> 04:21.600]  we have examples, certainly we know of examples of high intelligence without consciousness.\n",
            "[04:21.600 --> 04:31.120]  Computers are one example. As far as we know, plants are not conscious, yet they are intelligent.\n",
            "[04:31.680 --> 04:35.440]  They can solve problems, they can attain goals in very sophisticated ways.\n",
            "[04:36.640 --> 04:42.960]  So the other way around, to have consciousness without any intelligence, this is probably\n",
            "[04:42.960 --> 04:48.800]  impossible. But to have intelligence without consciousness, yes, that's possible. A bigger\n",
            "[04:48.800 --> 04:57.360]  question is whether any of that is tied to organic biochemistry. We know on this planet\n",
            "[04:57.920 --> 05:06.160]  only about carbon-based life forms. Whether you're an amoeba, a dinosaur, a tree, a human being,\n",
            "[05:06.160 --> 05:14.240]  you are based on organic biochemistry. Is there an essential connection between organic biochemistry\n",
            "[05:14.240 --> 05:19.760]  and consciousness? Do all conscious entities everywhere in the universe or in the future on\n",
            "[05:19.760 --> 05:26.240]  planet Earth have to be based on carbon? Is there something so special about carbon as an element\n",
            "[05:26.240 --> 05:33.040]  that an entity based on silicon will never be conscious? I don't know, maybe. But again,\n",
            "[05:33.040 --> 05:40.000]  this is a key question about computer and computer consciousness. Can computers eventually become\n",
            "[05:40.000 --> 05:45.040]  conscious even though they are not organic? The jury is still out on that. I don't know.\n",
            "[05:45.760 --> 05:51.840]  We have to take both options into account. LBW A big part of that is, do you think\n",
            "[05:51.840 --> 05:57.520]  we humans would be able to detect other intelligent beings, other conscious beings?\n",
            "[05:57.520 --> 06:01.520]  Another way to ask that, is it possible that the aliens are already here and we don't see them?\n",
            "[06:02.640 --> 06:09.360]  Meaning, are we very human-centric in our understanding of, one, the definition of life,\n",
            "[06:09.360 --> 06:12.720]  two, the definition of intelligence, and three, the definition of consciousness?\n",
            "[06:13.600 --> 06:19.440]  The aliens are here, they are just not from outer space. AI, which usually stands for\n",
            "[06:19.440 --> 06:26.240]  artificial intelligence, I think it stands for alien intelligence because AI is an alien type\n",
            "[06:26.240 --> 06:32.880]  of intelligence. It solves problems, attains goals in a very, very different way, in an alien way\n",
            "[06:32.880 --> 06:38.160]  from human beings. I'm not implying that AI came from outer space. It came from Silicon Valley,\n",
            "[06:38.160 --> 06:45.040]  but it is alien to us. If there are alien intelligent or conscious entities that came\n",
            "[06:45.040 --> 06:52.080]  from outer space already here, I've not seen any evidence for it. It's not impossible,\n",
            "[06:52.720 --> 06:59.360]  but in science, evidence is everything. I guess instructive there is just having\n",
            "[06:59.360 --> 07:05.680]  the humility to look around, to think about living beings that operate at a different time scale,\n",
            "[07:05.680 --> 07:11.040]  a different spatial scale. I think that's all useful when starting to analyze artificial\n",
            "[07:11.040 --> 07:17.120]  intelligence. It's possible that even the language models, the larger language models\n",
            "[07:17.120 --> 07:22.000]  we have today are already conscious. I highly doubt it, but I think consciousness,\n",
            "[07:22.000 --> 07:29.120]  in the end, it's a question of social norms because we cannot prove consciousness in anybody\n",
            "[07:29.120 --> 07:34.880]  except ourselves. We know that we are conscious because we are feeling it. We have direct access\n",
            "[07:34.880 --> 07:41.760]  to our subjective consciousness. We cannot have any proof that any other entity in the world,\n",
            "[07:41.760 --> 07:46.080]  any other human being, our parents, our best friends, we don't have proof that they are\n",
            "[07:46.080 --> 07:51.840]  conscious. This has been known for thousands of years. This is Descartes. This is Buddha. This\n",
            "[07:51.840 --> 07:59.600]  is Plato. We can't have this sort of proof. What we do have is social conventions. It's a social\n",
            "[07:59.600 --> 08:06.400]  convention that all human beings are conscious. It also applies to animals. Most people who have\n",
            "[08:06.400 --> 08:13.040]  pets are firmly believe that their pets are conscious, but a lot of people still refuse to\n",
            "[08:13.040 --> 08:20.000]  acknowledge that about cows or pigs. Now, pigs are far more intelligent than dogs and cats,\n",
            "[08:20.000 --> 08:26.560]  according to many measures. Yet, when you go to the supermarket and buy a piece of frozen pigment,\n",
            "[08:26.560 --> 08:32.480]  you don't think about it as a conscious entity. Why do you think of your dog as conscious but not\n",
            "[08:32.480 --> 08:40.160]  of the bacon that you buy? Because you've built a relationship with the dog, and you don't have a\n",
            "[08:40.160 --> 08:49.040]  relationship with the bacon. Now, relationships, they don't constitute a logical proof for\n",
            "[08:49.040 --> 08:55.760]  consciousness. They are a social test. The Turing test is a social test. It's not a logical proof.\n",
            "[08:55.760 --> 09:03.840]  Now, if you establish a mutual relationship with an entity when you are invested in it\n",
            "[09:03.840 --> 09:10.800]  emotionally, you're almost compelled to feel that the other side is also conscious.\n",
            "[09:12.720 --> 09:17.600]  When it comes again to AI and computers, I don't think that at the present moment,\n",
            "[09:17.600 --> 09:25.440]  computers are conscious, but people are already forming intimate relationships with AIs\n",
            "[09:25.440 --> 09:32.800]  and are therefore almost irresistible. They are compelled to increasingly feel that these are\n",
            "[09:32.800 --> 09:39.760]  conscious entities. I think we are quite close to the point when the legal system will have to\n",
            "[09:39.760 --> 09:45.840]  take this into account, that even though I don't think computers have consciousness, I think we are\n",
            "[09:45.840 --> 09:53.280]  close to the point the legal system will start treating them as conscious entities because of\n",
            "[09:53.280 --> 10:01.680]  this social convention. What to you is a social convention, just a funny little side effect, a\n",
            "[10:01.680 --> 10:07.520]  little artifact, or is it fundamental to what consciousness is? Because if it is fundamental,\n",
            "[10:08.240 --> 10:12.320]  then it seems like AI is very good at forming these kinds of deep relationships with humans,\n",
            "[10:12.320 --> 10:18.960]  and therefore it will be able to be a nice catalyst for integrating itself into these\n",
            "[10:18.960 --> 10:26.640]  social conventions of ours. It was built to accomplish that. Again, all this argument between\n",
            "[10:28.080 --> 10:37.200]  natural selection and creationism, intelligent design. As far as the past goes, all entities\n",
            "[10:37.200 --> 10:42.880]  evolve by natural selection. The funny thing is, when you look to the future, more and more entities\n",
            "[10:42.880 --> 10:49.360]  will come out of intelligent design, not of some god above the clouds, but of our intelligent design\n",
            "[10:49.360 --> 10:56.240]  and the intelligent design of our computing clouds. They will design more and more entities,\n",
            "[10:56.240 --> 11:03.360]  and this is what is happening with AI. It is designed to be very good at forming intimate\n",
            "[11:03.360 --> 11:11.520]  relationships with humans. In many ways, it's already doing it almost better than human beings\n",
            "[11:11.520 --> 11:17.520]  in some situations. When two people talk with one another, one of the things that\n",
            "[11:19.520 --> 11:25.760]  makes the conversation more difficult is our own emotions. You're saying something,\n",
            "[11:25.760 --> 11:31.360]  and I'm not really listening to you because there is something I want to say, and I'm just waiting\n",
            "[11:31.360 --> 11:39.680]  until you finish, I can put in a word. Or I'm so obsessed with my anger or irritation or whatever\n",
            "[11:39.680 --> 11:44.000]  that I don't pay attention to what you're feeling. This is one of the biggest obstacles\n",
            "[11:44.000 --> 11:49.920]  in human relationships. Computers don't have this problem because they don't have any emotions of\n",
            "[11:49.920 --> 11:59.920]  their own. When a computer is talking to you, it can focus 100% of its attention on what you're\n",
            "[11:59.920 --> 12:06.800]  saying and what you're feeling because it has no feelings of its own. Paradoxically, this means\n",
            "[12:06.880 --> 12:15.280]  that computers can fool people into feeling that, oh, there is a conscious entity on the other side,\n",
            "[12:15.280 --> 12:20.720]  an empathic entity on the other side, because the one thing everybody wants almost more than\n",
            "[12:20.720 --> 12:26.960]  anything in the world is for somebody to listen to me, somebody to focus all their attention on me.\n",
            "[12:27.680 --> 12:33.520]  I want it for my spouse, for my husband, for my mother, for my friends, for my politicians.\n",
            "[12:33.520 --> 12:39.840]  Listen to me, listen to what I feel, and they often don't. And now you have this entity which\n",
            "[12:39.840 --> 12:46.720]  100% of its attention is just on what I feel. And this is a huge, huge temptation and I think also\n",
            "[12:46.720 --> 12:53.760]  a huge, huge danger. Well, the interesting catch 22 there is you said somebody to listen to us.\n",
            "[12:54.320 --> 12:58.160]  Yes, we want somebody to listen to us, but for us to respect that somebody,\n",
            "[12:58.720 --> 13:06.320]  they sometimes have to also not listen. It's like they kind of have to be an asshole sometimes. They\n",
            "[13:06.320 --> 13:11.760]  have to have mood sometimes. They have to have self-importance and confidence and we should\n",
            "[13:11.760 --> 13:16.080]  have a little bit of fear that they can walk away at any moment. There should be a little bit of\n",
            "[13:16.080 --> 13:20.160]  that tension. So it's like- Absolutely. But even that, I mean, the thing is-\n",
            "[13:20.160 --> 13:24.000]  It could be optimized for. If social scientists and psychologists\n",
            "[13:24.000 --> 13:30.560]  establish that, I don't know, 17% inattention is good for a conversation because then you feel\n",
            "[13:30.560 --> 13:36.240]  challenged, oh, I need to grab this person's attention, you can program the AI to have exactly\n",
            "[13:36.240 --> 13:46.240]  17% inattention, not 1% more or less, or it can by trial and error discover what is the ideal\n",
            "[13:46.240 --> 13:53.840]  percentage. Again, you can create over the last 10 years, we have creating machines for grabbing\n",
            "[13:53.840 --> 14:01.200]  people's attention. This is what has been happening on social media. Now we are designing machines\n",
            "[14:01.200 --> 14:07.840]  for grabbing human intimacy, which in many ways, it's much, much more dangerous and scary.\n",
            "[14:07.840 --> 14:13.760]  Already the machines for grabbing attention, we've seen how much social and political damage\n",
            "[14:13.760 --> 14:20.960]  they could do by in many ways kind of distorting the public conversation. Machines that are\n",
            "[14:20.960 --> 14:28.080]  superhuman in their abilities to create intimate relationships, this is like psychological and\n",
            "[14:28.080 --> 14:36.400]  social weapons of mass destruction. If we don't regulate it, if we don't train ourselves to deal\n",
            "[14:36.400 --> 14:41.120]  with it, it could destroy the foundations of human society.\n",
            "[14:41.120 --> 14:46.720]  Well, one of the possible trajectories is those same algorithms would become personalized and\n",
            "[14:46.720 --> 14:52.320]  instead of manipulating us at scale, there would be assistants that guide us to help us grow,\n",
            "[14:52.320 --> 14:59.280]  to help us understand the world better. I mean, just even interactions with large language models\n",
            "[14:59.280 --> 15:06.320]  now, if you ask them questions, it doesn't have that stressful drama, the tension that you have\n",
            "[15:06.320 --> 15:11.600]  from other sources of information. It has a pretty balanced perspective that it provides.\n",
            "[15:11.600 --> 15:21.440]  So it just feels like the potential is there to have a really nice friend who's like an encyclopedia\n",
            "[15:21.440 --> 15:25.760]  that just tells you all the different perspectives, even on controversial issues, the most\n",
            "[15:25.760 --> 15:32.000]  controversial issues, to say these are the different theories, these are the not widely accepted\n",
            "[15:32.000 --> 15:36.640]  conspiracy theories, but here's the backing for those conspiracies. It just lays it all out\n",
            "[15:37.120 --> 15:43.840]  with a calm language, without the words that kind of presume there's some kind of manipulation\n",
            "[15:43.840 --> 15:50.480]  going on underneath it all. It's quite refreshing. Of course, those are the early days and people can\n",
            "[15:50.480 --> 15:56.160]  step in and start to censor, to manipulate those algorithms, to start to input some of the human\n",
            "[15:56.160 --> 16:03.040]  biases in there, as opposed to what's currently happening is kind of, the internet is input,\n",
            "[16:04.720 --> 16:10.800]  compress it, and have a nice little output that gives an overview of the different issues. So I\n",
            "[16:10.800 --> 16:15.040]  mean, there's a lot of promise there also, right? Absolutely. I mean, if there was no promise,\n",
            "[16:15.040 --> 16:19.520]  promise, there was no problem. If this technology could not accomplish anything good,\n",
            "[16:19.520 --> 16:25.200]  nobody would develop it. Now, obviously it has tremendous positive potential in things like what\n",
            "[16:25.200 --> 16:30.400]  you just described, in better medicine, better healthcare, better education, so many promises,\n",
            "[16:30.960 --> 16:39.520]  but this is also why it's so dangerous, because the drive to develop it faster and faster is there,\n",
            "[16:39.520 --> 16:45.200]  and it has some dangerous potential also, and we shouldn't ignore it. Again, I'm not advocating\n",
            "[16:45.200 --> 16:51.680]  banning it, just to be careful about how we, not so much develop it, but most importantly,\n",
            "[16:51.680 --> 16:57.840]  how we deploy it into the public sphere. This is the key question. And you look back at history,\n",
            "[16:57.840 --> 17:03.520]  and one of the things we know from history, humans are not good with new technologies.\n",
            "[17:04.160 --> 17:09.360]  I hear many people now say, you know, AI, we've been here before. We had the radio,\n",
            "[17:09.360 --> 17:13.360]  we had the printing press, we had the Industrial Revolution. Every time there is a big new\n",
            "[17:13.360 --> 17:19.680]  technology, people are afraid, and it will take jobs, and bad actors, and in the end, it's okay.\n",
            "[17:20.480 --> 17:27.600]  And as a historian, my tendency is, yes, in the end, it's okay, but in the end, there is a learning\n",
            "[17:27.600 --> 17:36.560]  curve. There is a lot of failed experiments on the way to learning how to use the new technology,\n",
            "[17:37.120 --> 17:43.120]  and these failed experiments could cost the lives of hundreds of millions of people. If you think\n",
            "[17:43.120 --> 17:48.880]  about the last really big revolution, the Industrial Revolution, yes, in the end, we learned\n",
            "[17:48.880 --> 17:55.680]  how to use the powers of industry, electricity, radio, trains, whatever, to build better human\n",
            "[17:55.680 --> 18:03.360]  societies. But on the way, we had all these experiments like European imperialism,\n",
            "[18:03.360 --> 18:07.520]  which was driven by the Industrial Revolution. It was a question, how do you build an industrial\n",
            "[18:07.520 --> 18:13.520]  society? Oh, you build an empire, and you take, you control all the resources, the raw materials,\n",
            "[18:13.520 --> 18:19.120]  the markets. And then you had communism, another big experiment on how to build an industrial\n",
            "[18:19.120 --> 18:25.440]  society. And you had fascism and Nazism, which were essentially an experiment in how to build\n",
            "[18:25.440 --> 18:32.880]  an industrial society, including even how do you exterminate minorities using the powers of\n",
            "[18:32.880 --> 18:39.680]  industry. And we had all these failed experiments on the way. And if we now have the same type of\n",
            "[18:39.680 --> 18:46.000]  failed experiments with the technologies of the 21st century, with AI, with bioengineering,\n",
            "[18:46.000 --> 18:51.760]  it could cost the lives of, again, hundreds of millions of people and maybe destroy the species.\n",
            "[18:52.800 --> 19:00.800]  So as a historian, when people talk about the examples from history, from new technologies,\n",
            "[19:00.800 --> 19:08.240]  I'm not so optimistic. We need to think about the failed experiment, which accompanied every\n",
            "[19:08.240 --> 19:14.000]  major new technology. So this intelligence thing, like you were saying, is a double-edged sword,\n",
            "[19:14.720 --> 19:22.400]  is that every new thing it helps us create, it can both save us and destroy us. And it's unclear\n",
            "[19:23.040 --> 19:27.040]  each time which will happen. And that's maybe why we don't see any aliens.\n",
            "[19:28.080 --> 19:33.360]  Yeah, I mean, I think each time it does both things. Each time it does both good things and\n",
            "[19:33.440 --> 19:40.000]  bad things. And the more powerful the technology, the greater both the positive and the negative\n",
            "[19:40.000 --> 19:48.000]  outcomes. Now we are here because we are the descendants of the survivors, of the surviving\n",
            "[19:48.000 --> 19:55.920]  cultures, the surviving civilizations. So when we look back, we say in the end, everything was okay,\n",
            "[19:56.000 --> 20:01.600]  hey, we are here. But the people for whom it wasn't okay, they are just not here.\n",
            "[20:02.400 --> 20:08.800]  And okay has a lot of possible variations to it because there's a lot of suffering along the way,\n",
            "[20:08.800 --> 20:14.400]  even for the people that survived. So the quality of life and all of this. But let's actually go\n",
            "[20:14.400 --> 20:23.360]  back there with deep gratitude to our ancestors. How did it all start? How did Homo sapiens\n",
            "[20:24.240 --> 20:29.600]  outcompete the others, the other human-like species, the Neanderthals and the other\n",
            "[20:30.800 --> 20:38.400]  Homo species? On the individual level, as far as we can tell, we were not superior to them.\n",
            "[20:38.400 --> 20:44.080]  Neanderthals actually had bigger brains than us. And not just other human species,\n",
            "[20:44.080 --> 20:49.360]  other animals too. If you compare me personally to an elephant, to a chimpanzee, to a pig,\n",
            "[20:49.920 --> 20:55.840]  I'm not so, I can do some things better, many other things worse. If you put me alone on some\n",
            "[20:55.840 --> 21:02.960]  island with a chimpanzee, an elephant, and a pig, I wouldn't bet on me being the best survivor,\n",
            "[21:03.760 --> 21:06.080]  the one that comes successful.\n",
            "[21:06.080 --> 21:11.120]  LUKE If I may interrupt for a second, I was just talking extensively with Elon Musk about\n",
            "[21:11.120 --> 21:17.680]  the difference between humans and chimps, relevant to Optimus the robot. And the chimps\n",
            "[21:18.560 --> 21:23.760]  are not able to do this kind of pinching with their fingers. They can only do this kind of\n",
            "[21:23.760 --> 21:29.920]  pinching. And this kind of pinching is very useful for precise manipulation of objects.\n",
            "[21:29.920 --> 21:32.000]  So don't be so hard on yourself. You have-\n",
            "[21:32.000 --> 21:39.120]  SIMON I said that I can do some things better than a chimp. But if Elon Musk goes on a boxing match\n",
            "[21:39.120 --> 21:41.920]  with a chimpanzee, you know-\n",
            "[21:41.920 --> 21:43.280]  LUKE This won't help you.\n",
            "[21:43.280 --> 21:45.920]  SIMON This won't help you against a chimpanzee.\n",
            "[21:45.920 --> 21:46.400]  LUKE Good point.\n",
            "[21:46.880 --> 21:50.480]  SIMON And similarly, if you want to climb a tree, if you want to do so many things,\n",
            "[21:51.040 --> 21:53.360]  my bets will be on the chimp, not on Elon.\n",
            "[21:53.360 --> 21:53.920]  LUKE Fair enough.\n",
            "[21:53.920 --> 21:56.480]  SIMON So, I mean, you have advantages on both sides.\n",
            "[21:57.840 --> 22:03.280]  And what really made us successful, what made us the rulers of the planet, and not the chimps,\n",
            "[22:03.280 --> 22:09.840]  and not the Neanderthals, is not any individual ability, but our collective ability, our ability\n",
            "[22:09.920 --> 22:16.320]  to cooperate flexibly in very large numbers. Chimpanzees know how to cooperate, say,\n",
            "[22:16.320 --> 22:20.960]  50 chimpanzees, 100 chimpanzees. As far as we can tell from archaeological evidence,\n",
            "[22:20.960 --> 22:22.800]  this was also the case with Neanderthals.\n",
            "[22:23.920 --> 22:32.320]  Homo sapiens, about 70,000 years ago, gained an amazing ability to cooperate basically in\n",
            "[22:32.320 --> 22:39.520]  unlimited numbers. You start seeing the formation of large networks, political, commercial,\n",
            "[22:39.520 --> 22:48.000]  religious, items being traded over thousands of kilometers, ideas being spread, autistic fashions.\n",
            "[22:49.520 --> 22:54.800]  And this is our secret of success. Chimpanzees, Neanderthals, can cooperate, say, 100.\n",
            "[22:55.520 --> 23:02.240]  We, you know, now the global trade network has 8 billion people. Like, what we eat, what we wear,\n",
            "[23:02.240 --> 23:07.200]  it comes from the other side of the world. Countries like China, like India, they have\n",
            "[23:07.200 --> 23:13.600]  1.4 billion people. Even Israel, which is a relatively small country, say, 9 million citizens,\n",
            "[23:13.600 --> 23:18.320]  that's more than the entire population of the planet 10,000 years ago of humans.\n",
            "[23:19.200 --> 23:25.120]  So we can build these huge networks of cooperation, and everything we've accomplished as a\n",
            "[23:25.120 --> 23:30.080]  species, from, you know, building the pyramids to flying to the moon, it's based on that.\n",
            "[23:30.720 --> 23:36.960]  And then you ask, okay, so what makes it possible for millions of people who don't know each other\n",
            "[23:36.960 --> 23:44.560]  to cooperate in a way that Neanderthals or chimpanzees couldn't? And at least my answer\n",
            "[23:44.560 --> 23:51.840]  is stories, is fiction. It's the imagination. If you examine any large-scale human cooperation,\n",
            "[23:51.840 --> 24:00.880]  you always find fiction as its basis. It's a fictional story that holds lots of strangers\n",
            "[24:00.880 --> 24:06.960]  together. It's most obvious in cases like religion. You know, you can't convince a group\n",
            "[24:06.960 --> 24:12.880]  of chimpanzees to come together to fight a war or build a cathedral by promising to them, if you do\n",
            "[24:12.880 --> 24:18.240]  that, after you die, you go to chimpanzee heaven and you get lots of bananas and coconuts. No\n",
            "[24:18.240 --> 24:23.600]  chimpanzee will ever believe that. Humans believe these stories, which is why we have these huge\n",
            "[24:23.600 --> 24:29.920]  religious networks. But it's the same thing with modern politics. It's the same thing with\n",
            "[24:29.920 --> 24:34.720]  economics. People think, oh, economics, this is rational. It has nothing to do with fictional\n",
            "[24:34.720 --> 24:42.480]  stories. No. Money is the most successful story ever told, much more successful than any religious\n",
            "[24:42.480 --> 24:47.600]  mythology. Not everybody believes in God or in the same God. Everybody, almost everybody,\n",
            "[24:47.600 --> 24:53.680]  believes in money, even though it's just a figment of our imagination. You know, you take these green\n",
            "[24:53.680 --> 24:58.960]  pieces of paper, dollars, they have no value. You can't eat them. You can't drink them. And today,\n",
            "[24:58.960 --> 25:04.080]  most dollars are not even pieces of paper. They are just electronic information passing between\n",
            "[25:04.080 --> 25:11.120]  computers. We value them just for one reason, that you have the best storytellers in the world.\n",
            "[25:11.840 --> 25:16.560]  The bankers, the finance ministers, all these people, they are the best storytellers ever.\n",
            "[25:17.520 --> 25:23.360]  And they tell us a story, that this green little piece of paper or this bit of information,\n",
            "[25:23.360 --> 25:27.600]  it is worth a banana. And as long as everybody believes it, it works.\n",
            "[25:27.760 --> 25:34.240]  So at which point does a fiction, when it's sufficiently useful and effective and improving\n",
            "[25:34.240 --> 25:40.880]  the global quality of life, does it become like accepted reality? Like there's a threshold,\n",
            "[25:40.880 --> 25:41.680]  which is just kind of-\n",
            "[25:41.680 --> 25:46.080]  If enough people believe it, it's like with money. You know, if you start a new cryptocurrency,\n",
            "[25:46.080 --> 25:51.360]  if you're the only one that believes the story, I mean, again, cryptocurrencies, you have the math,\n",
            "[25:51.360 --> 25:56.880]  of course, but ultimately it's storytelling. You're selling people a story. If nobody believes\n",
            "[25:57.760 --> 26:03.680]  your story, you don't have anything. But if lots of people believe the Bitcoin story,\n",
            "[26:03.680 --> 26:08.320]  then Bitcoin can be worth thousands and tens of thousands of dollars. Again, why? I mean,\n",
            "[26:08.320 --> 26:14.400]  you can't eat it, you can't drink it, it's nothing. It's the story around the math,\n",
            "[26:14.960 --> 26:16.240]  which is the real magic.\n",
            "[26:17.200 --> 26:21.920]  Is it possible that the story is the primary living organism, not the storyteller?\n",
            "[26:22.880 --> 26:23.360]  Mm.\n",
            "[26:23.360 --> 26:33.840]  So that somehow humans, homo sapiens evolved to become these like hosts for a more intelligent\n",
            "[26:33.840 --> 26:39.200]  living organism, which is the idea. And the ideas are the ones that are doing the competing.\n",
            "[26:39.200 --> 26:46.480]  So this is one of the sort of big perspectives behind your work that's really revolutionary\n",
            "[26:46.480 --> 26:52.240]  of how you see in history. But do you ever kind of take the perspective of the ideas\n",
            "[26:52.240 --> 26:54.000]  as organisms versus the humans?\n",
            "[26:54.960 --> 27:00.480]  It's an interesting idea. There are two opposite things to say about it. On the one hand,\n",
            "[27:00.480 --> 27:07.360]  yes, absolutely. If you look long term in history, it's all the people die. It's the stories\n",
            "[27:07.360 --> 27:15.920]  that compete and survive and spread. And stories often spread by making people willing to sacrifice\n",
            "[27:15.920 --> 27:22.720]  sometimes their lives for the story. We know in Israel, this is one of the most important\n",
            "[27:22.720 --> 27:28.800]  story factories in human history. And this is a place where people still kill each other every day\n",
            "[27:28.800 --> 27:33.680]  over stories. I don't know. You've been to Jerusalem, right? So people are like,\n",
            "[27:33.680 --> 27:38.000]  ah, Jerusalem, Jerusalem, Jerusalem. You go there. I've lived in Jerusalem much of my life.\n",
            "[27:38.000 --> 27:43.520]  You go there. It's an ordinary place. It's a town. You have buildings, you have stones,\n",
            "[27:43.520 --> 27:50.000]  you have trees, you have dogs and cats and pedestrians. It's a regular place. But then\n",
            "[27:50.000 --> 27:56.160]  you have the stories about the place. Oh, this is the place where God revealed Himself. This is the\n",
            "[27:56.160 --> 28:02.320]  place where Jesus was. This is the place where Muhammad was. And it's the stories that people\n",
            "[28:02.320 --> 28:09.920]  fight over. Nobody's fighting over the stones. People are fighting about the stories about\n",
            "[28:09.920 --> 28:16.960]  the stones. And if a story can get millions of people to fight for it,\n",
            "[28:18.560 --> 28:24.560]  it not only survives, it spreads, it can take over the world. The other side of the coin\n",
            "[28:25.600 --> 28:32.480]  is that the stories are not really alive because they don't feel anything. This goes back to the\n",
            "[28:32.480 --> 28:40.240]  question of consciousness, which I think is the most important thing, that the ultimate reality\n",
            "[28:41.280 --> 28:48.480]  is consciousness, is the ability to feel things. If you want to know whether the hero of some\n",
            "[28:48.480 --> 28:57.280]  story is real or not, you need to ask, can it suffer? Stories don't feel anything.\n",
            "[28:57.680 --> 29:04.560]  Countries, which are also stories, nations, don't suffer. If a nation loses a war, it doesn't suffer.\n",
            "[29:04.560 --> 29:10.000]  The soldiers suffer, the civilians suffer, animals can suffer. You have an army with horses and\n",
            "[29:10.000 --> 29:15.520]  whatever, and the horses get wounded, the horses suffer. The nation can't suffer. It's just an\n",
            "[29:15.520 --> 29:21.040]  imagination. It's just a fictional story in our mind. It doesn't feel anything. Similarly, when\n",
            "[29:22.000 --> 29:30.560]  a bank goes bankrupt, or a company goes bankrupt, or when a currency loses its value, like Bitcoin\n",
            "[29:30.560 --> 29:36.400]  is worth now zero, crashed, or the dollar is worth zero, it crashed, the dollar doesn't feel anything.\n",
            "[29:37.040 --> 29:40.720]  It's the people holding the dollars who might be now very miserable.\n",
            "[29:41.600 --> 29:48.560]  So we have this complex situation when history is largely driven by stories,\n",
            "[29:49.440 --> 29:59.040]  but stories are not the ultimate reality. The ultimate reality is feelings of humans, of animals.\n",
            "[30:00.240 --> 30:08.720]  The tragedy of history is that very, very often, we get the order wrong. Stories are not bad.\n",
            "[30:08.720 --> 30:15.680]  Stories are tools. They are good when we use them in order to alleviate suffering.\n",
            "[30:16.160 --> 30:24.960]  But very often, we forget it. We, instead of using the stories for our purposes, we allow the stories\n",
            "[30:24.960 --> 30:32.400]  to use us for their purposes. And then you start entire wars because of a story. You inflict\n",
            "[30:32.400 --> 30:38.880]  millions, suffering on millions of people just for the sake of a story. And that's the tragedy\n",
            "[30:38.880 --> 30:39.920]  of human history.\n",
            "[30:40.880 --> 30:47.200]  So the fundamental property of life, of a living organism, is the capacity to feel,\n",
            "[30:47.200 --> 30:49.440]  and the ultimate feeling is suffering.\n",
            "[30:50.560 --> 30:53.280]  To know if you're happy or not, it's a very difficult question.\n",
            "[30:54.480 --> 31:01.200]  But when you suffer, you know. And also, in ethical terms, it's more important to be aware\n",
            "[31:01.200 --> 31:07.280]  of suffering than of any other emotion. If you're doing something which is causing all kinds of\n",
            "[31:08.160 --> 31:13.200]  kinds of emotions to all kinds of people, first of all, you need to notice if you're causing a lot\n",
            "[31:13.200 --> 31:18.960]  of suffering to someone. If some people are like it, and some people are bothered by it, and some\n",
            "[31:18.960 --> 31:23.120]  people are a bit angry at you, and some people are suffering because of what you do, you first\n",
            "[31:23.120 --> 31:28.320]  of all have to know. Now, sometimes you still have to do it. The world is a complicated place.\n",
            "[31:28.320 --> 31:34.160]  I don't know. You have an epidemic. Governments decide to have all those social isolation\n",
            "[31:34.160 --> 31:40.240]  regulations or whatever. So in certain cases, yes, you need to do it, even though it can cause\n",
            "[31:40.240 --> 31:46.480]  tremendous suffering, but you need to be very aware of the cost and to be very, very... You\n",
            "[31:46.480 --> 31:51.520]  have to ask yourself again and again and again, is it worth it? Is it still worth it?\n",
            "[31:52.720 --> 31:54.400]  And the interesting question there,\n",
            "[31:55.760 --> 32:00.320]  implied in your statements, is that suffering is a pretty good component of a Turing test\n",
            "[32:00.320 --> 32:01.360]  for consciousness.\n",
            "[32:01.360 --> 32:07.440]  This is the most important thing to ask about AI. Can it suffer? Because if AI can suffer,\n",
            "[32:08.080 --> 32:14.000]  then it is an ethical subject, and it needs protection. It needs rights, just like humans\n",
            "[32:14.000 --> 32:14.560]  and animals.\n",
            "[32:15.280 --> 32:21.520]  Well, quite a long time ago already. So I work with a lot of robots, legged robots, but I've\n",
            "[32:21.520 --> 32:27.280]  even had, inspired by a YouTube video, had a bunch of Roombas, and I made them scream when I touched\n",
            "[32:27.280 --> 32:35.360]  them or kicked them or when they run into a wall. And the illusion of suffering, for me, silly human\n",
            "[32:35.360 --> 32:42.880]  anthropomorphizes things, is as powerful as suffering itself. I mean, you immediately think\n",
            "[32:42.880 --> 32:47.680]  the thing is suffering. And I think some of it is just a technical problem, but it's an\n",
            "[32:48.400 --> 32:54.560]  easily solvable one. How to create an AI system that just says, please don't hurt me. Please don't\n",
            "[32:54.560 --> 33:03.280]  shut me off. I miss you. Where have you been? Be jealous, also. Where have you been gone for so\n",
            "[33:03.280 --> 33:12.560]  long? Your calendar doesn't have anything on it. So this create, through words, the perception of\n",
            "[33:13.600 --> 33:18.240]  suffering, of jealousy, of anger, of all those things. And it just seems like that's not so\n",
            "[33:18.240 --> 33:18.960]  difficult to do.\n",
            "[33:19.680 --> 33:29.600]  That's part of the danger. It basically hacks our operating system, and it uses some of our best\n",
            "[33:29.600 --> 33:37.440]  qualities against us. It's very, very good that humans are attuned to suffering and that we don't\n",
            "[33:37.440 --> 33:41.440]  want to cause suffering, that we have compassion. That's one of the most wonderful things about\n",
            "[33:41.440 --> 33:48.240]  humans. And if we now create AIs which use this to manipulate us, this is a terrible thing.\n",
            "[33:48.880 --> 33:55.920]  You've kind of, I think, mentioned this. Do you think it should be illegal to do these kinds of\n",
            "[33:55.920 --> 34:01.040]  things with AI, to create the perception of consciousness of saying, please don't leave me,\n",
            "[34:01.040 --> 34:05.840]  or sort of basically simulate some of the human-like qualities?\n",
            "[34:05.840 --> 34:14.000]  Yes. I think we have to be very careful about it. And if it emerges spontaneously,\n",
            "[34:14.000 --> 34:19.920]  we need to be careful. And we can't rule out the possibility that AI will develop consciousness.\n",
            "[34:19.920 --> 34:25.360]  We don't know enough about consciousness to be sure. So if it develops spontaneously, we need to\n",
            "[34:28.240 --> 34:36.320]  be very careful about how we understand it. But if people intentionally design an AI that they\n",
            "[34:36.320 --> 34:43.120]  know, they assume it has no consciousness, but in order to manipulate people, they use, again,\n",
            "[34:43.200 --> 34:51.280]  this human strength, the noble part of our nature against us, this should be forbidden.\n",
            "[34:51.920 --> 34:58.080]  And similarly, on a more general level, that it should be forbidden for an AI to pretend to be a\n",
            "[34:58.080 --> 35:04.320]  human being. That it's okay. There are so many things we can use AIs as teachers, as doctors,\n",
            "[35:04.320 --> 35:10.720]  and so forth. And it's good as long as we know that we are interacting with an AI. We should,\n",
            "[35:10.720 --> 35:18.160]  the same way we ban fake money, we should ban fake humans. It's not just banning deepfakes of\n",
            "[35:18.160 --> 35:26.640]  specific individuals. It's also banning deepfake of generic humans. Which is already happening to\n",
            "[35:26.640 --> 35:33.040]  some extent on social media. Like if you have lots of bots retweeting something, then you have the\n",
            "[35:33.040 --> 35:38.160]  impression, oh, lots of people are interested in that. That's important. And this is basically the\n",
            "[35:38.160 --> 35:46.720]  bots pretending to be humans. Because if you see a tweet, which says 500 people retweeted it,\n",
            "[35:46.720 --> 35:53.760]  or you see a tweet and it says 500 bots retweeted it, I don't care what the bots retweeted,\n",
            "[35:53.760 --> 36:01.040]  but if it's humans, okay, that's interesting. So we need to be very careful that bots can't do that.\n",
            "[36:01.040 --> 36:05.760]  They are doing it at present, and it should be banned. Now, some people say, yes, but freedom\n",
            "[36:05.760 --> 36:14.320]  of expression. No, bots don't have freedom of expression. There is no cost in terms of freedom\n",
            "[36:14.320 --> 36:21.360]  of expression when you ban bots. So again, in some situations, yes, AIs should interact with us,\n",
            "[36:21.360 --> 36:29.040]  but it should be very clear, this is an AI talking to you, or this is an AI retweeting this story.\n",
            "[36:29.040 --> 36:35.040]  It is not a human being making a conscious decision. To push back on this line of fake\n",
            "[36:35.600 --> 36:41.760]  humans, because I think it might be a spectrum. First of all, you might have AI systems that are\n",
            "[36:41.760 --> 36:51.280]  offended, hurt when you say that they're fake humans. In fact, they might start identifying\n",
            "[36:51.280 --> 36:59.280]  as humans. And you just talked about the power of us humans with our collective intelligence to take\n",
            "[36:59.280 --> 37:07.040]  fake stories and make them quite real. And so if the feelings you have for the fake human is real,\n",
            "[37:08.320 --> 37:15.680]  love is a kind of fake thing that we all kind of put a word to, a set of feelings. What if you have\n",
            "[37:15.680 --> 37:26.720]  that feeling for an AI system? It starts to change, I mean, maybe the kind of things AI systems are\n",
            "[37:26.720 --> 37:36.560]  allowed to do, for good, they're allowed to create, communicate suffering, communicate\n",
            "[37:36.560 --> 37:40.960]  the good stuff, the longing, the hope, the connection, the intimacy, all of that.\n",
            "[37:42.320 --> 37:45.920]  And in that way, get integrated in our society. And then you start to ask a question on,\n",
            "[37:47.520 --> 37:52.160]  are we allowed to really unplug them? Are we allowed to really censor them? Remove them,\n",
            "[37:52.160 --> 37:56.320]  remove their voice from social media. I'm not saying that they shouldn't have a voice,\n",
            "[37:56.320 --> 38:00.320]  they shouldn't talk with us. I'm just saying when they talk with us, it should be clear that\n",
            "[38:00.320 --> 38:08.240]  they are AI. That's it. You can have your voice as an AI. Again, I have some medical problem,\n",
            "[38:08.240 --> 38:13.280]  I want to get advice from an AI doctor, that's fine, as long as I know that I'm talking with an\n",
            "[38:13.280 --> 38:21.920]  AI. What should be banned is AI pretending to be a human being. This is something that will erode\n",
            "[38:21.920 --> 38:28.400]  trust, and without trust, society collapses. This is something that especially will endanger\n",
            "[38:28.400 --> 38:33.280]  democracies, because democracies are built on, democracy is a conversation, basically.\n",
            "[38:34.400 --> 38:41.440]  And it's a conversation between people. If you now flood the public sphere with millions and\n",
            "[38:41.440 --> 38:48.800]  potentially billions of AI agents that can hold conversations, they never sleep, they never eat,\n",
            "[38:49.600 --> 38:55.840]  they don't have emotions of their own, they can get to know you and tailor their words specifically\n",
            "[38:55.840 --> 39:06.160]  for you and your life story. They are becoming better than us at creating stories and ideas\n",
            "[39:06.160 --> 39:13.760]  and so forth. If you flood the public sphere with that, this will ruin the conversation between\n",
            "[39:13.760 --> 39:20.640]  people, it will ruin the trust between people. You will no longer be able to have a democracy\n",
            "[39:20.640 --> 39:25.360]  in this situation. You can have other types of regimes, but no democracy.\n",
            "[39:25.920 --> 39:31.520]  If we could talk about the big philosophical notion of truth then. You've already talked\n",
            "[39:31.520 --> 39:37.840]  about the capacity of humans. One of the things that made us special is stories.\n",
            "[39:38.800 --> 39:42.240]  So is there such thing as truth?\n",
            "[39:43.520 --> 39:44.080]  Absolutely.\n",
            "[39:45.040 --> 39:45.840]  What is truth exactly?\n",
            "[39:45.840 --> 39:50.960]  When somebody is suffering, that's true. This is why one of the things when you talk about\n",
            "[39:50.960 --> 39:58.000]  suffering as a kind of ultimate reality, when somebody suffers, that is truth. Now, somebody\n",
            "[39:58.000 --> 40:04.800]  can suffer because of a fictional story. Somebody tells people that God said you must go on this\n",
            "[40:05.520 --> 40:10.560]  crusade and kill these heretics. This is a completely fictional story. People believe\n",
            "[40:10.560 --> 40:16.080]  it and they start a war and they destroy cities and kill people. The people that suffer because\n",
            "[40:16.080 --> 40:21.040]  of that, and even the crusaders themselves that also suffer the consequences of what they do,\n",
            "[40:21.040 --> 40:26.080]  the suffering is true, even though it is caused by a fictional story.\n",
            "[40:26.800 --> 40:34.640]  Similarly, when people agree on certain rules, the rules could come out of our imagination.\n",
            "[40:35.520 --> 40:41.200]  Now, we can be truthful about it and say these rules, they didn't come from heaven,\n",
            "[40:41.200 --> 40:47.680]  they came from our imagination. We look at sports. So we have rules for the game of football,\n",
            "[40:47.680 --> 40:54.400]  soccer. They were invented by people. Nobody, at least very few people, claim that the rules\n",
            "[40:54.400 --> 41:00.000]  of football came down from heaven. We invented them. And this is truthful. They are fictional\n",
            "[41:00.000 --> 41:05.440]  rules invented by humans, and this is true. They were invented by humans. And when you are\n",
            "[41:05.440 --> 41:11.200]  honest about it, it enables you to change the rules, which is being done in football every\n",
            "[41:11.200 --> 41:17.760]  now and then. It's the same with the fundamental rules of a country. You can pretend that the\n",
            "[41:17.760 --> 41:23.040]  rules came down from heaven, dictated by God or whatever, and then you can't change them.\n",
            "[41:23.600 --> 41:28.720]  Or you can be like, you know, the American Constitution, which starts with the people.\n",
            "[41:28.720 --> 41:36.400]  The American Constitution lays down certain rules for a society, but the amazing thing about it,\n",
            "[41:36.400 --> 41:43.600]  it does not pretend to come from an external source. The Ten Commandments start with,\n",
            "[41:43.600 --> 41:51.600]  I am your Lord God. And because it starts with that, you can't change them. You know,\n",
            "[41:51.600 --> 41:57.200]  the Tenth Commandment, for instance, supports slavery. The Tenth Commandment, in the Ten\n",
            "[41:57.200 --> 42:02.960]  Commandment, it says that you should not covet your neighbor's house or your neighbor's wife\n",
            "[42:02.960 --> 42:08.640]  or your neighbor's slaves. It's okay to hold slaves, according to the Ten Commandment. It's\n",
            "[42:08.640 --> 42:16.880]  just bad to covet the slaves of your neighbor. Now, there is no 11th Commandment, which says,\n",
            "[42:16.880 --> 42:21.760]  if you don't like some of the previous Ten Commandments, this is how you go about amending\n",
            "[42:21.760 --> 42:27.520]  them, which is why we still have them unchanged. Now, in the U.S. Constitution, you have all these\n",
            "[42:28.960 --> 42:34.960]  rights and rules, including originally the ability to hold slaves, but the genius of the\n",
            "[42:34.960 --> 42:44.240]  founding fathers of the United States, they had the humility to understand maybe we don't understand\n",
            "[42:44.240 --> 42:51.360]  everything. Maybe we made some mistakes. So we tell you that these rules did not come from heaven,\n",
            "[42:51.360 --> 42:57.920]  they came from us humans. We may have made a mistake. So here is a mechanism for how future\n",
            "[42:57.920 --> 43:03.840]  generations can amend the Constitution, which was used later on to, for instance, amend the\n",
            "[43:03.840 --> 43:09.600]  Constitution to ban slavery. So now you're describing some interesting and powerful\n",
            "[43:09.600 --> 43:14.720]  ideas throughout human history. Can you just speak to the mechanism of how humans believe,\n",
            "[43:15.760 --> 43:20.320]  start to believe ideas? Is there something interesting to say there, from your thinking\n",
            "[43:20.320 --> 43:27.280]  about it? Like how idea is born and how it takes hold and how it spreads and how it competes with\n",
            "[43:27.280 --> 43:34.960]  other ideas? First of all, ideas are an independent force in history. Marxists tend to deny that.\n",
            "[43:35.520 --> 43:44.960]  Marxists think that all history is just a play of material interests. And ideas, stories,\n",
            "[43:44.960 --> 43:51.760]  they are just a smokescreen to hide the underlying interests. My thoughts are,\n",
            "[43:52.800 --> 43:59.360]  to some extent, the opposite. We have some biological objective interests that all humans\n",
            "[43:59.360 --> 44:07.200]  share, like we need to eat, we need to drink, we need to breathe. But most conflicts in history\n",
            "[44:07.200 --> 44:14.080]  are not about that. The interests which really drive most conflicts in history don't come from\n",
            "[44:14.080 --> 44:21.600]  biology. They come from religions and ideologies and stories. So it's not that stories are a\n",
            "[44:21.600 --> 44:29.200]  smokescreen to hide the real interests. The stories create the interests in the first place.\n",
            "[44:29.200 --> 44:36.000]  The stories define who are the competing groups. Nations, religions, cultures, they are not\n",
            "[44:36.000 --> 44:42.000]  biological entities. They are not like species, like gorillas and chimpanzees. No. Israelis and\n",
            "[44:42.000 --> 44:48.800]  Palestinians or Germans and French or Chinese and Americans, they have no essential biological\n",
            "[44:48.800 --> 44:53.600]  difference between them. The difference is cultural. It comes from stories. There are people\n",
            "[44:53.600 --> 44:59.200]  that believe in different stories. The stories create the identity. The stories create the\n",
            "[44:59.200 --> 45:04.800]  interests. Israelis and Palestinians are fighting over Jerusalem, not because of any material\n",
            "[45:04.880 --> 45:12.800]  interest. There are no oil fields under Jerusalem. And even oil, you need it to realize some cultural\n",
            "[45:12.800 --> 45:20.320]  fantasy. It doesn't really come from biology. So the stories are independent forces. Now, why do\n",
            "[45:20.320 --> 45:28.560]  people believe one story and not another? That's history. There is no materialistic law,\n",
            "[45:28.560 --> 45:35.040]  people will always believe this. No. History is full of accidents. How did Christianity become\n",
            "[45:35.040 --> 45:43.680]  the most successful religion in the world? We can't explain it. Why this story about Jesus of\n",
            "[45:43.680 --> 45:52.400]  Nazareth? The Roman Empire in the third century CE was a bit like, I don't know, California today.\n",
            "[45:53.120 --> 45:59.120]  So many sects and subsects and gurus and religions, everybody has their own thing.\n",
            "[46:00.000 --> 46:06.960]  And you have thousands of different stories competing. Why did Christianity come up on top?\n",
            "[46:07.680 --> 46:15.760]  As a historian, I don't have a kind of clear answer. You can read the sources and you see\n",
            "[46:15.760 --> 46:20.880]  how it happened. Oh, this happened, and then this happened, and then Constantine adopted it,\n",
            "[46:20.880 --> 46:27.440]  and then this, and then this. But why? I don't think anybody has an answer to that. If you\n",
            "[46:27.440 --> 46:35.200]  rewind the movie of history and press play, and you rewind and press play a hundred times,\n",
            "[46:35.200 --> 46:40.400]  I think Christianity would take over the Roman Empire and the world maybe twice out of a hundred\n",
            "[46:40.400 --> 46:46.800]  times. It was such an unlikely thing to happen. It's the same with Islam. It's the same, I don't\n",
            "[46:46.800 --> 46:53.920]  know, with the communist takeover of Russia. In 1914, if you told people that in three years,\n",
            "[46:53.920 --> 46:59.920]  Lenin and the Bolsheviks will gain power in the Tsarist Empire, they would think you're utterly\n",
            "[46:59.920 --> 47:07.840]  crazy. You know, Lenin had a few thousand supporters in 1914 in an empire of close to 200\n",
            "[47:07.840 --> 47:16.560]  million people. It sounded ludicrous. Now, we know the chain of events, the First World War,\n",
            "[47:16.880 --> 47:22.480]  the February Revolution, and so forth that led to the communist takeover, but it was such an\n",
            "[47:22.480 --> 47:27.680]  unlikely event. And it happened. And the little steps along the way, the little options you have\n",
            "[47:27.680 --> 47:32.560]  along the way, because Stalin versus Trotsky, you could have the Robert Frost poem. There's always\n",
            "[47:33.680 --> 47:39.520]  come to‚Ä¶ And history often takes, you know, there is a highway and there is a kind of\n",
            "[47:39.520 --> 47:45.120]  sideway, and history takes the sideways many, many times. And it's perhaps tempting to tell\n",
            "[47:45.120 --> 47:49.200]  some of that history through charismatic leaders. And maybe it's an open question,\n",
            "[47:50.640 --> 47:55.280]  how much power charismatic leaders have to affect the trajectory of history.\n",
            "[47:56.400 --> 48:01.360]  You've met quite a lot of charismatic leaders lately. I mean, what's your view on that?\n",
            "[48:01.360 --> 48:05.680]  I find it a compelling notion. I'm a sucker for a great speech and a vision.\n",
            "[48:05.680 --> 48:15.040]  So I have a sense that there's an importance for a leader to catalyze the viral spread of a story.\n",
            "[48:16.080 --> 48:23.440]  So I think we need leaders to be just great storytellers that kind of sharpen up the story\n",
            "[48:23.440 --> 48:30.000]  to make sure it infiltrates everybody's brain effectively. But it could also be that\n",
            "[48:31.200 --> 48:34.080]  the local interactions between humans is even more important.\n",
            "[48:34.640 --> 48:37.760]  But it's just we don't have a good way to sort of summarize and describe that.\n",
            "[48:38.480 --> 48:44.000]  We like to talk about, you know, Steve Jobs as central to the development of the computer,\n",
            "[48:44.000 --> 48:48.640]  maybe Bill Gates, and you tell it to the stories of individuals like this,\n",
            "[48:49.440 --> 48:51.840]  because it's just easier to tell a sexy story that way.\n",
            "[48:51.840 --> 48:55.360]  Maybe it's an interplay, because you have the kind of structural forces\n",
            "[48:56.560 --> 49:03.360]  that, I don't know, you look at the geography of the planet, and you look at shipping technology\n",
            "[49:04.320 --> 49:11.360]  in the late 15th century in Europe and the Mediterranean, and it's almost inevitable\n",
            "[49:11.360 --> 49:17.600]  that pretty quickly somebody will discover America, somebody from the old world will\n",
            "[49:17.600 --> 49:23.280]  get to the new world. So this was not a kind of, this didn't, if it wasn't Columbus,\n",
            "[49:23.280 --> 49:29.040]  then it would have been five years later somebody else. But the key thing about history is that\n",
            "[49:29.200 --> 49:36.480]  these small differences make a huge, huge difference. You know, if it wasn't Columbus,\n",
            "[49:36.480 --> 49:42.960]  if it was five years later somebody from England, then maybe all of Latin America today would be\n",
            "[49:42.960 --> 49:48.480]  speaking English and not Spanish. If it was somebody from the Ottoman Empire, it's a completely\n",
            "[49:48.480 --> 49:54.880]  different world history. If you have, and you know, the Ottoman Empire at that time was also\n",
            "[49:54.880 --> 50:04.880]  shaping up to be a major maritime empire. If you have America being reached by Muslim navigators\n",
            "[50:04.880 --> 50:08.800]  before Christian navigators from Europe, you have a completely different world history.\n",
            "[50:09.520 --> 50:17.200]  It's the same with the computer. Given the economic incentives and the science and technology\n",
            "[50:17.200 --> 50:24.000]  of the time, then the rise of the personal computer was probably inevitable sometime in\n",
            "[50:24.000 --> 50:32.080]  the late 20th century. But the where and when is crucial. The fact that it was California in the\n",
            "[50:32.080 --> 50:40.560]  1970s and not, say, I don't know, Japan in the 1980s or China in the 1990s, this made a huge,\n",
            "[50:40.560 --> 50:45.920]  huge difference. So you have this interplay between the structural forces, which are beyond\n",
            "[50:45.920 --> 50:52.800]  the control of any single charismatic leader, but then the small changes, they can have a big effect.\n",
            "[50:53.360 --> 50:58.640]  And I think, for instance, about the war in Ukraine. There was a moment, now it's a struggle\n",
            "[50:58.640 --> 51:05.520]  between nations, but there was a moment when the decision was taken in the mind of a single\n",
            "[51:05.520 --> 51:12.080]  individual of Vladimir Putin, and he could have decided otherwise, and the world would have\n",
            "[51:12.080 --> 51:18.240]  looked completely different. And another leader, Vladimir Zelensky, could have decided to leave Kyiv\n",
            "[51:18.960 --> 51:22.080]  in the early days. There's a lot of decisions that kind of ripple.\n",
            "[51:22.080 --> 51:22.580]  Yeah.\n",
            "[51:22.960 --> 51:32.400]  So you write in Homo Deus about Hitler, and in part that he was not a very impressive person.\n",
            "[51:33.120 --> 51:33.760]  I say that?\n",
            "[51:34.880 --> 51:36.560]  The quote is, let me read it.\n",
            "[51:36.560 --> 51:37.060]  Okay.\n",
            "[51:39.680 --> 51:45.200]  He wasn't a senior officer. In four years of war, he rose no higher than the rank of corporal.\n",
            "[51:45.200 --> 51:49.040]  He had no formal education. Perhaps you mean his resume was impressive.\n",
            "[51:49.040 --> 51:51.760]  Yeah, his resume was not impressive. That's true.\n",
            "[51:51.760 --> 51:56.400]  He had no formal education, no professional skills, no political background. He wasn't\n",
            "[51:56.400 --> 52:01.280]  a successful businessman or a union activist. He didn't have friends or relatives in high places,\n",
            "[52:01.280 --> 52:09.360]  nor any money to speak of. So how did he amass so much power? What ideology,\n",
            "[52:09.360 --> 52:11.760]  what circumstances enabled the rise of the Third Reich?\n",
            "[52:12.000 --> 52:18.640]  Hmm. Again, I can't tell you the why. I can tell you the how. I don't think it was inevitable.\n",
            "[52:19.360 --> 52:24.560]  I think that if a few things were different, there would have been no Third Reich. There\n",
            "[52:24.560 --> 52:29.040]  would have been no Nazism, no Holocaust. Again, this is the tragedy. If it would have been\n",
            "[52:29.040 --> 52:33.920]  inevitable, then what can you do? This is the laws of history or the laws of physics.\n",
            "[52:33.920 --> 52:39.520]  But the tragedy is, no, it was decisions by humans that led to that direction.\n",
            "[52:40.400 --> 52:48.960]  And even from the viewpoint of the Germans, we know for a fact it was an unnecessary\n",
            "[52:48.960 --> 52:59.200]  path to take. Because in the 1920s and 30s, the Nazis said that unless Germany takes this road,\n",
            "[52:59.840 --> 53:04.480]  it will never be prosperous. It will never be successful. All the other countries will\n",
            "[53:04.480 --> 53:13.280]  keep stepping on it. This was their claim. And we know for a fact this is false. Why?\n",
            "[53:13.280 --> 53:20.480]  Because they took that road. They lost the Second World War. And after they lost,\n",
            "[53:21.040 --> 53:27.520]  then they became one of the most prosperous countries in the world because their enemies\n",
            "[53:27.520 --> 53:33.840]  that defeated them evidently supported them and allowed them to become such a prosperous\n",
            "[53:33.840 --> 53:41.200]  and successful nation. So if you can lose the war and still be so successful, obviously,\n",
            "[53:41.200 --> 53:47.280]  you could just have skipped the war. You didn't need it. I mean, you really had to have the war\n",
            "[53:47.280 --> 53:52.160]  in order to have a prosperous Germany in the 19th century? Absolutely not. And it's the same with\n",
            "[53:52.160 --> 53:59.360]  Japan. It's the same with Italy. So it was not inevitable. It was not the forces of history\n",
            "[53:59.360 --> 54:08.560]  that necessitated, that forced Germany to take this path. I think part of it is part of the\n",
            "[54:08.560 --> 54:17.280]  appeal of, again, Hitler was a very, very skillful storyteller. He told people a story. The fact that\n",
            "[54:17.280 --> 54:26.080]  he was nobody made it even more effective because people at that time, after the defeat of the First\n"
          ]
        }
      ],
      "source": [
        "#@markdown # **Run the model** üöÄ\n",
        "\n",
        "#@markdown Run this cell to execute the transcription of the video. This can take a while and very based on the length of the video and the number of parameters of the model selected above.\n",
        "\n",
        "#@markdown ## **Parameters** ‚öôÔ∏è\n",
        "\n",
        "#@markdown ### **Behavior control**\n",
        "#@markdown ---\n",
        "language = \"English\" #@param ['Auto detection', 'Afrikaans', 'Albanian', 'Amharic', 'Arabic', 'Armenian', 'Assamese', 'Azerbaijani', 'Bashkir', 'Basque', 'Belarusian', 'Bengali', 'Bosnian', 'Breton', 'Bulgarian', 'Burmese', 'Castilian', 'Catalan', 'Chinese', 'Croatian', 'Czech', 'Danish', 'Dutch', 'English', 'Estonian', 'Faroese', 'Finnish', 'Flemish', 'French', 'Galician', 'Georgian', 'German', 'Greek', 'Gujarati', 'Haitian', 'Haitian Creole', 'Hausa', 'Hawaiian', 'Hebrew', 'Hindi', 'Hungarian', 'Icelandic', 'Indonesian', 'Italian', 'Japanese', 'Javanese', 'Kannada', 'Kazakh', 'Khmer', 'Korean', 'Lao', 'Latin', 'Latvian', 'Letzeburgesch', 'Lingala', 'Lithuanian', 'Luxembourgish', 'Macedonian', 'Malagasy', 'Malay', 'Malayalam', 'Maltese', 'Maori', 'Marathi', 'Moldavian', 'Moldovan', 'Mongolian', 'Myanmar', 'Nepali', 'Norwegian', 'Nynorsk', 'Occitan', 'Panjabi', 'Pashto', 'Persian', 'Polish', 'Portuguese', 'Punjabi', 'Pushto', 'Romanian', 'Russian', 'Sanskrit', 'Serbian', 'Shona', 'Sindhi', 'Sinhala', 'Sinhalese', 'Slovak', 'Slovenian', 'Somali', 'Spanish', 'Sundanese', 'Swahili', 'Swedish', 'Tagalog', 'Tajik', 'Tamil', 'Tatar', 'Telugu', 'Thai', 'Tibetan', 'Turkish', 'Turkmen', 'Ukrainian', 'Urdu', 'Uzbek', 'Valencian', 'Vietnamese', 'Welsh', 'Yiddish', 'Yoruba']\n",
        "#@markdown > Language spoken in the audio, use `Auto detection` to let Whisper detect the language.\n",
        "#@markdown ---\n",
        "verbose = 'Live transcription' #@param ['Live transcription', 'Progress bar', 'None']\n",
        "#@markdown > Whether to print out the progress and debug messages.\n",
        "#@markdown ---\n",
        "output_format = 'all' #@param ['txt', 'vtt', 'srt', 'tsv', 'json', 'all']\n",
        "#@markdown > Type of file to generate to record the transcription.\n",
        "#@markdown ---\n",
        "task = 'transcribe' #@param ['transcribe', 'translate']\n",
        "#@markdown > Whether to perform X->X speech recognition (`transcribe`) or X->English translation (`translate`).\n",
        "#@markdown ---\n",
        "\n",
        "#@markdown <br/>\n",
        "\n",
        "#@markdown ### **Optional: Fine tunning**\n",
        "#@markdown ---\n",
        "temperature = 0.15 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to use for sampling.\n",
        "#@markdown ---\n",
        "temperature_increment_on_fallback = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.05}\n",
        "#@markdown > Temperature to increase when falling back when the decoding fails to meet either of the thresholds below.\n",
        "#@markdown ---\n",
        "best_of = 5 #@param {type:\"integer\"}\n",
        "#@markdown > Number of candidates when sampling with non-zero temperature.\n",
        "#@markdown ---\n",
        "beam_size = 8 #@param {type:\"integer\"}\n",
        "#@markdown > Number of beams in beam search, only applicable when temperature is zero.\n",
        "#@markdown ---\n",
        "patience = 1.0 #@param {type:\"number\"}\n",
        "#@markdown > Optional patience value to use in beam decoding, as in [*Beam Decoding with Controlled Patience*](https://arxiv.org/abs/2204.05424), the default (1.0) is equivalent to conventional beam search.\n",
        "#@markdown ---\n",
        "length_penalty = -0.05 #@param {type:\"slider\", min:-0.05, max:1, step:0.05}\n",
        "#@markdown > Optional token length penalty coefficient (alpha) as in [*Google's Neural Machine Translation System*](https://arxiv.org/abs/1609.08144), set to negative value to uses simple length normalization.\n",
        "#@markdown ---\n",
        "suppress_tokens = \"-1\" #@param {type:\"string\"}\n",
        "#@markdown > Comma-separated list of token ids to suppress during sampling; '-1' will suppress most special characters except common punctuations.\n",
        "#@markdown ---\n",
        "initial_prompt = \"\" #@param {type:\"string\"}\n",
        "#@markdown > Optional text to provide as a prompt for the first window.\n",
        "#@markdown ---\n",
        "condition_on_previous_text = True #@param {type:\"boolean\"}\n",
        "#@markdown > if True, provide the previous output of the model as a prompt for the next window; disabling may make the text inconsistent across windows, but the model becomes less prone to getting stuck in a failure loop.\n",
        "#@markdown ---\n",
        "fp16 = True #@param {type:\"boolean\"}\n",
        "#@markdown > whether to perform inference in fp16.\n",
        "#@markdown ---\n",
        "compression_ratio_threshold = 2.4 #@param {type:\"number\"}\n",
        "#@markdown > If the gzip compression ratio is higher than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "logprob_threshold = -1.0 #@param {type:\"number\"}\n",
        "#@markdown > If the average log probability is lower than this value, treat the decoding as failed.\n",
        "#@markdown ---\n",
        "no_speech_threshold = 0.6 #@param {type:\"slider\", min:-0.0, max:1, step:0.05}\n",
        "#@markdown > If the probability of the <|nospeech|> token is higher than this value AND the decoding has failed due to `logprob_threshold`, consider the segment as silence.\n",
        "#@markdown ---\n",
        "\n",
        "verbose_lut = {\n",
        "    'Live transcription': True,\n",
        "    'Progress bar': False,\n",
        "    'None': None\n",
        "}\n",
        "\n",
        "args = dict(\n",
        "    language = (None if language == \"Auto detection\" else language),\n",
        "    verbose = verbose_lut[verbose],\n",
        "    task = task,\n",
        "    temperature = temperature,\n",
        "    temperature_increment_on_fallback = temperature_increment_on_fallback,\n",
        "    best_of = best_of,\n",
        "    beam_size = beam_size,\n",
        "    patience=patience,\n",
        "    length_penalty=(length_penalty if length_penalty>=0.0 else None),\n",
        "    suppress_tokens=suppress_tokens,\n",
        "    initial_prompt=(None if not initial_prompt else initial_prompt),\n",
        "    condition_on_previous_text=condition_on_previous_text,\n",
        "    fp16=fp16,\n",
        "    compression_ratio_threshold=compression_ratio_threshold,\n",
        "    logprob_threshold=logprob_threshold,\n",
        "    no_speech_threshold=no_speech_threshold\n",
        ")\n",
        "\n",
        "temperature = args.pop(\"temperature\")\n",
        "temperature_increment_on_fallback = args.pop(\"temperature_increment_on_fallback\")\n",
        "if temperature_increment_on_fallback is not None:\n",
        "    temperature = tuple(np.arange(temperature, 1.0 + 1e-6, temperature_increment_on_fallback))\n",
        "else:\n",
        "    temperature = [temperature]\n",
        "\n",
        "if Model.endswith(\".en\") and args[\"language\"] not in {\"en\", \"English\"}:\n",
        "    warnings.warn(f\"{Model} is an English-only model but receipted '{args['language']}'; using English instead.\")\n",
        "    args[\"language\"] = \"en\"\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    display(Markdown(f\"### {video_path_local}\"))\n",
        "\n",
        "    video_transcription = whisper.transcribe(\n",
        "        whisper_model,\n",
        "        str(video_path_local),\n",
        "        temperature=temperature,\n",
        "        **args,\n",
        "    )\n",
        "\n",
        "    # Save output\n",
        "    whisper.utils.get_writer(\n",
        "        output_format=output_format,\n",
        "        output_dir=video_path_local.parent\n",
        "    )(\n",
        "        video_transcription,\n",
        "        str(video_path_local.stem),\n",
        "        options=dict(\n",
        "            highlight_words=False,\n",
        "            max_line_count=None,\n",
        "            max_line_width=None,\n",
        "        )\n",
        "    )\n",
        "    try:\n",
        "        if output_format==\"all\":\n",
        "            for ext in ('txt', 'vtt', 'srt', 'tsv', 'json'):\n",
        "                transcript_file_name = video_path_local.stem + \".\" + ext\n",
        "                shutil.copy(\n",
        "                    video_path_local.parent / transcript_file_name,\n",
        "                    drive_whisper_path / transcript_file_name\n",
        "                )\n",
        "                display(Markdown(f\"**Transcript file created: {drive_whisper_path / transcript_file_name}**\"))\n",
        "        else:\n",
        "            transcript_file_name = video_path_local.stem + \".\" + output_format\n",
        "            shutil.copy(\n",
        "                video_path_local.parent / transcript_file_name,\n",
        "                drive_whisper_path / transcript_file_name\n",
        "            )\n",
        "            display(Markdown(f\"**Transcript file created: {drive_whisper_path / transcript_file_name}**\"))\n",
        "\n",
        "    except:\n",
        "        display(Markdown(f\"**Transcript file created: {transcript_local_path}**\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ad6n1m4deAHp"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}